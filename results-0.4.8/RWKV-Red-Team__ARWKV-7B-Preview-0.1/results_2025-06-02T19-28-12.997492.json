{
  "results": {
    "niah_single_1": {
      "alias": "niah_single_1",
      "4096,none": 0.236,
      "4096_stderr,none": "N/A"
    }
  },
  "group_subtasks": {
    "niah_single_1": []
  },
  "configs": {
    "niah_single_1": {
      "task": "niah_single_1",
      "tag": [
        "longcxt"
      ],
      "custom_dataset": "def niah_single_1(**kwargs):\n    seq_lengths = kwargs.pop(\"max_seq_lengths\", DEFAULT_SEQ_LENGTHS)\n    return download_dataset(\n        generate_samples(\n            get_haystack(type_haystack=\"repeat\"),\n            max_seq_length=seq,\n            template=TEMPLATE,\n            type_haystack=\"repeat\",\n            type_needle_k=\"words\",\n            type_needle_v=\"numbers\",\n            num_samples=500,\n            TOKENIZER=get_tokenizer(**kwargs),\n        )\n        for seq in seq_lengths\n    )\n",
      "dataset_path": "",
      "dataset_name": "",
      "test_split": "test",
      "doc_to_text": "{{input}}",
      "doc_to_target": "{{outputs}}",
      "unsafe_code": false,
      "process_results": "def process_results(doc: dict, results: list[str]) -> dict[str, float]:\n    # hacky: set all other lengths to -1\n    metrics = {str(length): -1.0 for length in DEFAULT_SEQ_LENGTHS}\n    input_len = doc[\"max_length\"]\n    pred = postprocess_pred(results)\n    score = string_match_all(pred, [doc[\"outputs\"]])\n    metrics[str(input_len)] = score\n    return metrics\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "4096",
          "aggregation": "def aggregate_metrics(metrics: list[float]) -> float:\n    res = [x for x in metrics if x != -1]\n    if not res:\n        # we don't have any samples with this length\n        return -1\n    return sum(res) / len(res)\n",
          "higher_is_better": true
        },
        {
          "metric": "8192",
          "aggregation": "def aggregate_metrics(metrics: list[float]) -> float:\n    res = [x for x in metrics if x != -1]\n    if not res:\n        # we don't have any samples with this length\n        return -1\n    return sum(res) / len(res)\n",
          "higher_is_better": true
        },
        {
          "metric": "16384",
          "aggregation": "def aggregate_metrics(metrics: list[float]) -> float:\n    res = [x for x in metrics if x != -1]\n    if not res:\n        # we don't have any samples with this length\n        return -1\n    return sum(res) / len(res)\n",
          "higher_is_better": true
        },
        {
          "metric": "32768",
          "aggregation": "def aggregate_metrics(metrics: list[float]) -> float:\n    res = [x for x in metrics if x != -1]\n    if not res:\n        # we don't have any samples with this length\n        return -1\n    return sum(res) / len(res)\n",
          "higher_is_better": true
        },
        {
          "metric": "65536",
          "aggregation": "def aggregate_metrics(metrics: list[float]) -> float:\n    res = [x for x in metrics if x != -1]\n    if not res:\n        # we don't have any samples with this length\n        return -1\n    return sum(res) / len(res)\n",
          "higher_is_better": true
        },
        {
          "metric": "131072",
          "aggregation": "def aggregate_metrics(metrics: list[float]) -> float:\n    res = [x for x in metrics if x != -1]\n    if not res:\n        # we don't have any samples with this length\n        return -1\n    return sum(res) / len(res)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 128,
        "until": []
      },
      "repeats": 1,
      "should_decontaminate": false,
      "gen_prefix": "{{gen_prefix}}",
      "metadata": {
        "version": 1.0,
        "pretrained": "RWKV-Red-Team/ARWKV-7B-Preview-0.1",
        "trust_remote_code": true,
        "max_seq_lengths": [
          4096
        ]
      }
    }
  },
  "versions": {
    "niah_single_1": 1.0
  },
  "n-shot": {
    "niah_single_1": 0
  },
  "higher_is_better": {
    "niah_single_1": {
      "4096": true,
      "8192": true,
      "16384": true,
      "32768": true,
      "65536": true,
      "131072": true
    }
  },
  "n-samples": {
    "niah_single_1": {
      "original": 500,
      "effective": 500
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=RWKV-Red-Team/ARWKV-7B-Preview-0.1,trust_remote_code=True",
    "model_num_parameters": 8291057152,
    "model_dtype": "torch.float16",
    "model_revision": "main",
    "model_sha": "55d2dd9e88ef411ae9732594e525d0a28dc76b4f",
    "batch_size": "2",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "08d426a",
  "date": 1748891523.5946214,
  "pretty_env_info": "'NoneType' object has no attribute 'splitlines'",
  "transformers_version": "4.49.0",
  "lm_eval_version": "0.4.8",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|endoftext|>",
    "151643"
  ],
  "tokenizer_eos_token": [
    "<|im_end|>",
    "151645"
  ],
  "tokenizer_bos_token": [
    null,
    "None"
  ],
  "eot_token_id": 151645,
  "max_length": 32768,
  "task_hashes": {},
  "model_source": "hf",
  "model_name": "RWKV-Red-Team/ARWKV-7B-Preview-0.1",
  "model_name_sanitized": "RWKV-Red-Team__ARWKV-7B-Preview-0.1",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
  "chat_template_sha": "cd8e9439f0570856fd70470bf8889ebd8b5d1107207f67a5efb46e342330527f",
  "start_time": 15566353.012988808,
  "end_time": 15567324.387884103,
  "total_evaluation_time_seconds": "971.3748952951282"
}